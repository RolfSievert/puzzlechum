#! /usr/bin/env python
# -*- coding: utf-8 -*-
# vim:fenc=utf-8

"""
Runs tests on provided problem id.
"""

import argparse
import subprocess

from pathlib import Path

RED = '\x1b[38;5;3m'
BLUE = '\x1b[38;5;2m'
GREEN = '\x1b[38;5;1m'
YELLOW = '\x1b[38;5;4m'
BOLD = '\x1b[1m'
DIMMED = '\x1b[2m'
ITALICS = '\x1b[3m'
NULL = '\x1b[0m'

ACCEPTED_SRC_SUFFIXES = [
    '.cpp',
    '.rs',
    '.py'
]

# call .resolve() to make symlinking of scripts possible
PROBLEM_ROOT = Path(__file__).resolve().parent / 'problems'
TESTS_ROOT = Path(__file__).resolve().parent / 'tests'
TMP_PATH = Path(__file__).resolve().parent / '.runtest_tmp'

HYPERFINE = 'hyperfine'

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('problem_name')
    parser.add_argument('-b', '--benchmark', action='store_true', help='print minimal time execution benchmarks using hyperfine')
    parser.add_argument('-n', '--no-cleanup', action='store_true', help="leave compilation and output files in '.runtest_tmp/'")
    return parser.parse_args()

def get_ins_and_ans(test_dirs):
    ans = []
    ins = []

    for d in test_dirs:
        if d.is_dir():
            ans += d.glob('*.ans')
            ins += d.glob('*.in')

    # remove test files that don't have both .in and .ans
    ans = [a for a in ans if str(a.parent / a.stem) in [str(i.parent / i.stem) for i in ins]]
    ins = [i for i in ins if str(i.parent / i.stem) in [str(a.parent / a.stem) for a in ans]]

    ans = sorted(ans)
    ins = sorted(ins)

    return list(zip([str(x) for x in ins], [str(x) for x in ans]))

def check_create_tmp_dir():
    if not TMP_PATH.is_dir():
        try:
            TMP_PATH.mkdir()
        except (FileExistsError, FileNotFoundError) as err:
            print("Could not create {TMP_PATH}:", err)

def is_accepted_src_file(file_path, problem_name):
    return \
        file_path.is_file() \
        and file_path.name.startswith(problem_name) \
        and file_path.suffix in ACCEPTED_SRC_SUFFIXES

def get_source_files(problem_name):
    problem_dir = PROBLEM_ROOT / problem_name
    return [x for x in problem_dir.iterdir() if is_accepted_src_file(x, problem_name)]

def compile_and_get_test_command(source_file, problem_dir):
    # TODO extend to handle more languages
    output_name = f'{source_file.stem}_{source_file.suffix[1:]}'
    output_executable = TMP_PATH / output_name

    if source_file.suffix == '.cpp':
        # see https://open.kattis.com/help/cpp
        CC = f'g++ -g -O2 -std=gnu++17 -static -lrt -Wl,--whole-archive -lpthread -Wl,--no-whole-archive'
        CC += f' -o {output_executable} -I {problem_dir}'
    elif source_file.suffix == '.rs':
        # see https://open.kattis.com/help/rust
        CC = f'rustc -o {output_executable} --crate-type bin --edition=2018'
    elif source_file.suffix == '.py':
        # see https://open.kattis.com/languages/python3
        try:
            subprocess.run(["pypy3", "--version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        except:
            raise Exception('Executing python code requires `pypy3` (as that is what kattis does, see https://open.kattis.com/languages/python3)')

        CC = f'pypy3'
    else:
        raise Exception(f'Programming language not supported: {source_file.suffix}')

    output = None
    if source_file.suffix not in ('.py'):
        output = subprocess.run(f'{CC} {source_file}', shell=True, capture_output=True)

    if output and output.returncode != 0: # execution error
        print()
        print(f'{RED}{source_file} FAILED TO COMPILE!{NULL}')
        print(output.stderr.decode("utf-8"))
        return ''
    else:
        if output and output.stderr:
            print()
            print(f'{YELLOW}{source_file} has compile warnings:{NULL}')
            print(output.stderr.decode("utf-8"))

        if source_file.suffix == '.py':
            return f'pypy3 {source_file} < {"{}"} > {"{}"}'
        else:
            return f'{output_executable} < {"{}"} > {"{}"}'

def check_test(output_path, ans_path):
    # output logging
    stack_max_size = 16
    output_stack = []
    success = True

    """
    If all correct, don't print.

    Print stack of previously correct lines as green.

    Print wrong output and all following as red.

    Print expected output.
    """
    with open(output_path, 'r') as o_file, open(Path(ans_path), 'r') as a_file:
        for line, a_line in enumerate(a_file):
            # only check when input has a non-whitespace
            if not a_line.isspace():
                o_line = o_file.readline()
                while o_line.isspace():
                    o_line = o_file.readline()

                # lines are not the same (excluding whitespace)
                if a_line.split() != o_line.split():
                    success = False
                    for o in output_stack:
                        if o == output_stack[-1]:
                            print(f'{GREEN}{o}{NULL}')
                        else:
                            print(o)
                    print(f'{YELLOW}{ans_path}{NULL} failed at line {line+1}')
                    print(f'EXPECTED: {BLUE}{a_line.rstrip()}{NULL}')

                    print(f'{RED}{o_line}{o_file.read()}{NULL}')

                    # Print rest of expected output
                    a_dump = a_file.read()
                    if (a_dump):
                        print('ANSWERS:')
                        print(a_dump)

                # save output
                output_stack.append(o_line)
                if len(output_stack) > stack_max_size:
                    output_stack.pop(0)
        # When done reading answers, check that there are not any residual in output
        rest = o_file.read()
        if rest:
            success = False
            print(f'{YELLOW}{ans_path}{NULL} got trailing output:');
            print(f'{RED}{rest}{NULL}')
            print()

    return success

def benchmark_fastest(program_name, input_file, execution_cmd):
    test_name = Path(input_file).stem
    md_path = TMP_PATH / f'{program_name}_{test_name}.md'

    execution_without_input_output = execution_cmd[:execution_cmd.find(' < ')]

    subprocess.run(f"{HYPERFINE} --shell=none --export-markdown={str(md_path)} --command-name '{program_name} --> {test_name}' --input '{input_file}' '{execution_without_input_output}'", shell=True)

    with open(md_path, 'r') as f:
        lines = f.readlines()
        unit = lines[0].split('|')[3][6:][:-2].strip()
        stats = lines[2]
        fastest = stats.split('|')[3].strip()

        return f'{fastest} {unit}'

def run_and_print_benchmarks(benchmarks):
    benchmarks = sorted(benchmarks, key= lambda b: b[0] + b[1])
    names = sorted(set([x for x, _, _ in benchmarks]))
    tests = sorted(set([Path(x).stem for _, x, _ in benchmarks]))

    speeds = {}
    for i, (bench_name, input_file, execution_cmd) in enumerate(benchmarks):
        fastest = benchmark_fastest(bench_name, input_file, execution_cmd)

        speeds[(bench_name, Path(input_file).stem)] = fastest

    # build table template
    max_test_name_length = len(max(tests, key=len))
    table_template = '{0:' + str(max_test_name_length + 2) + '}'
    for i, _ in enumerate(names):
        table_template += '{' + str(i + 1) + ':10}'

    print(f'{BOLD}Fastest executions{NULL}')
    header = ['']
    for i, n in enumerate(names):
        problem_num = f' ({i + 1})'
        header.append(problem_num)
        print(f'{problem_num} {n}')
    print(table_template.format(*header))
    for t in tests:
        row = [t]
        for i, n in enumerate(names):
            s = ' '
            if (n, t) in speeds:
                s = speeds[(n, t)]
            row.append(s)

        print(table_template.format(*row))

def has_hyperfine():
    from shutil import which
    return which('hyperfine') is not None

def __main__():
    args = parse_args()
    problem_dir = PROBLEM_ROOT / args.problem_name

    test_dir = TESTS_ROOT / args.problem_name
    test_dirs = [test_dir, problem_dir / 'test', problem_dir / 'tests']
    ins_ans_pairs = get_ins_and_ans(test_dirs)

    check_create_tmp_dir()

    source_files = get_source_files(args.problem_name)
    sources_string = ', '.join(str(src) for src in source_files)
    print(f'{DIMMED}Compiling source files... [{sources_string}]{NULL}')

    test_commands = []
    for s in source_files:
        test_command = compile_and_get_test_command(s, problem_dir)
        if test_command:
            test_commands.append(test_command)

    if not sum(1 for _ in ins_ans_pairs):
        test_folder_paths = "', \n\t'".join(str(t) for t in test_dirs)
        print(f"No tests found! Make sure to add both .in and .ans files to any of the folders:\n\t'{test_folder_paths}'")
    else:

        # tuples of name, test name, execution command
        benchmarks = []
        tests_string = ', '.join(str(test) for test, _ in ins_ans_pairs)
        print(f'{DIMMED}Running tests... [{tests_string}]{NULL}')
        print()

        for i, test_command in enumerate(test_commands):
            src = source_files[i]
            success = True
            for in_file, ans_file in ins_ans_pairs:
                test_output_path = TMP_PATH / f'{src.stem}_{src.suffix[1:]}_{Path(in_file).stem}_output'
                execution_cmd = test_command.format(in_file, test_output_path)
                output = subprocess.run([f'({execution_cmd})'], shell=True, capture_output=True)
                if output.returncode != 0: # execution error
                    print()
                    print(f"{RED}{src} ERROR WHILE RUNNING TEST '{str(in_file)}'!{NULL}")
                    print(output.stderr.decode("utf-8"))
                    print(output.stdout.decode("utf-8"))
                    print('while running:')
                    print(f'{DIMMED}{execution_cmd}{NULL}')
                    exit(1)
                else: # no execution error
                    success = check_test(test_output_path, ans_file) and success

                    if args.benchmark:
                        benchmarks.append((src.name, in_file, execution_cmd))

            print(src)
            if success:
                print(f'{GREEN}  ✔ - PASSED{NULL}')
            else:
                print(f'{RED}  ✗ - FAILED{NULL}')

        if args.benchmark:
            print()

            if not has_hyperfine():
                print(f'{BOLD}hyperfine{NULL} is not installed on your system! It is required to benchmark tests.')
                exit(1)

            run_and_print_benchmarks(benchmarks)

    # cleanup
    if not args.no_cleanup:
        for x in TMP_PATH.iterdir():
            x.unlink()
        TMP_PATH.rmdir()

__main__()
